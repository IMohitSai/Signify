# -*- coding: utf-8 -*-
"""Signify.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-kU1i-eaVqMfNwEYUZPIumxWdUHME9wT

### SIGNIFY

Pre-processing the data
"""

from google.colab import drive
import os
from PIL import Image
import numpy as np

#Mount Google Drive
drive.mount('/content/drive')

#Define the path to the parent folder containing subfolders
folder_path = '/content/drive/MyDrive/ColabNotebooks/Signify/signtotextnew'  # Update the path if needed

#Create a directory for saving the preprocessed images
output_preprocessed_dir = '/content/preprocessed_signtotextnew_images'
os.makedirs(output_preprocessed_dir, exist_ok=True)

#Initialize lists to hold images and labels
images = []
labels = []

#Iterate through each subfolder (representing a letter)
for folder_name in os.listdir(folder_path):
    subfolder_path = os.path.join(folder_path, folder_name)

    #Check if it's a folder
    if os.path.isdir(subfolder_path):
        #Create subfolder in the output directory for saving processed images
        output_folder_path = os.path.join(output_preprocessed_dir, folder_name)
        os.makedirs(output_folder_path, exist_ok=True)

        #Iterate through all images in the subfolder
        for image_name in os.listdir(subfolder_path):
            image_path = os.path.join(subfolder_path, image_name)

            #Open and preprocess the image
            try:
                img = Image.open(image_path).convert('RGB')  # Convert image to RGB
                img = img.resize((128, 128))  # Resize the image to 128x128
                img_array = np.array(img) / 255.0  # Normalize pixel values to [0, 1]

                #Append the image and corresponding label to the lists
                images.append(img_array)
                labels.append(folder_name)  # Use folder name as the label (e.g., 'A', 'B', etc.)

                #Save the preprocessed image in the output directory
                img_save_path = os.path.join(output_folder_path, image_name)  # Save with the original name
                img.save(img_save_path)

            except Exception as e:
                print(f"Error processing {image_path}: {e}")

#Convert lists to NumPy arrays
X = np.array(images)  # Images array
y = np.array(labels)  # Labels array

print(f"Total images preprocessed: {X.shape[0]}")
print(f"Labels: {np.unique(y)}")

#X contains the preprocessed images, and y contains the corresponding labels.

!ls /content/drive/MyDrive/preprocessed_signtotextnew_images

"""LOADING THE PRE PROCESSED IMGAES"""

import os
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

#Load preprocessed images from the saved directory
preprocessed_dir = '/content/drive/MyDrive/ColabNotebooks/Signify/preprocessed_signtotextnew_images'  # Update the path as needed

images = []
for folder_name in os.listdir(preprocessed_dir):
    folder_path = os.path.join(preprocessed_dir, folder_name)
    if os.path.isdir(folder_path):
        for image_name in os.listdir(folder_path):
            image_path = os.path.join(folder_path, image_name)
            if os.path.isfile(image_path):  # Ensure it's a file
                img = Image.open(image_path)
                img_array = (np.array(img) - 127.5) / 127.5  # Normalize to [-1, 1]
                images.append(img_array)

#Convert to NumPy array for GAN training
images = np.array(images)

#Display the entire NumPy array
print(images)

#Display the first 5 image arrays
for i in range(min(5, len(images))):  # Display only up to 5 images
    print(f"Image {i+1} array:\n", images[i])

#Visualize the arrays as images (first 5)
for i in range(min(5, len(images))):
    plt.figure(figsize=(6, 6))
    plt.imshow((images[i] * 127.5 + 127.5).astype(np.uint8))  # Rescale back to [0, 255] range
    plt.axis('off')
    plt.title(f'Image {i+1}')
    plt.show()

#Display array values visually as heatmaps (first 5)
for i in range(min(5, len(images))):
    plt.figure(figsize=(6, 6))
    plt.imshow(images[i], cmap='viridis')  # Visualize the array values using a heatmap
    plt.colorbar()
    plt.title(f'Array Visualization {i+1}')
    plt.show()

"""# cloning DF GAN"""

!pip install transformers
!pip install sentencepiece

from google.colab import drive
drive.mount('/content/drive')

"""# Installing necessary packages and importing the model"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from PIL import Image
import os
from tqdm import tqdm
import numpy as np
from google.colab import drive

"""# Using BERT for dataset and training"""

import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import torchvision.transforms as transforms

class TextProcessor:
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertModel.from_pretrained('bert-base-uncased')
        self.model.eval()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

    def get_embedding(self, text):
        with torch.no_grad():
            inputs = self.tokenizer(text,
                                  return_tensors="pt",
                                  padding=True,
                                  truncation=True,
                                  max_length=512).to(self.device)
            outputs = self.model(**inputs)
            return outputs.last_hidden_state[:, 0, :]

class SignLanguageDataset(Dataset):
    def __init__(self, root_dir, text_processor):
        self.root_dir = root_dir
        self.text_processor = text_processor
        self.classes = sorted(os.listdir(root_dir))

        # Image transformation
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])

        self.images = []
        self.embeddings = []

        for class_name in self.classes:
            class_dir = os.path.join(root_dir, class_name)
            if os.path.isdir(class_dir):
                embedding = text_processor.get_embedding(f"The letter {class_name} in sign language")
                for img_name in os.listdir(class_dir):
                    if img_name.endswith(('.png', '.jpg', '.jpeg')):
                        self.images.append(os.path.join(class_dir, img_name))
                        self.embeddings.append(embedding)

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        embedding = self.embeddings[idx]

        # Load and transform image
        try:
            with Image.open(img_path).convert('RGB') as img:
                image = self.transform(img)
            return image, embedding
        except Exception as e:
            print(f"Error loading image {img_path}: {str(e)}")
            return torch.zeros((3, 256, 256)), embedding

class Generator(nn.Module):
    def __init__(self, bert_embedding_dim=768):
        super(Generator, self).__init__()

        self.embedding_processor = nn.Sequential(
            nn.Linear(bert_embedding_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 512)
        )

        self.main = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, 1, 0, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),

            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(True),

            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(True),

            nn.ConvTranspose2d(16, 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(8),
            nn.ReLU(True),

            nn.ConvTranspose2d(8, 3, 4, 2, 1, bias=False),  # Changed to 4x4 kernel with integer stride
            nn.Tanh()
        )

    def forward(self, bert_embedding):
        latent = self.embedding_processor(bert_embedding)
        latent = latent.view(-1, 512, 1, 1)
        return self.main(latent)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        self.main = nn.Sequential(
            nn.Conv2d(3, 16, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(16, 32, 4, 2, 1, bias=False),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(32, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(512, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input):
        return self.main(input).view(-1)

def train_model(dataloader, num_epochs=100, save_path='model_checkpoints'):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    generator = Generator().to(device)
    discriminator = Discriminator().to(device)

    criterion = nn.BCELoss()
    g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

    print("Starting Training Loop...")

    for epoch in range(num_epochs):
        for i, (real_images, bert_embeddings) in enumerate(tqdm(dataloader)):
            batch_size = real_images.size(0)
            real_images = real_images.to(device)
            bert_embeddings = bert_embeddings.squeeze(1).to(device)

            # Train Discriminator
            d_optimizer.zero_grad()
            label_real = torch.ones(batch_size, device=device)
            label_fake = torch.zeros(batch_size, device=device)

            output_real = discriminator(real_images).view(-1)
            d_loss_real = criterion(output_real, label_real)

            fake_images = generator(bert_embeddings)
            output_fake = discriminator(fake_images.detach()).view(-1)
            d_loss_fake = criterion(output_fake, label_fake)

            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            d_optimizer.step()

            # Train Generator
            g_optimizer.zero_grad()
            output_fake = discriminator(fake_images).view(-1)
            g_loss = criterion(output_fake, label_real)

            g_loss.backward()
            g_optimizer.step()

            if (i + 1) % 100 == 0:
                print(f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{len(dataloader)}], '
                      f'd_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')

        # Save models after each epoch
        if save_path:
            os.makedirs(save_path, exist_ok=True)
            torch.save({
                'epoch': epoch,
                'generator_state_dict': generator.state_dict(),
                'discriminator_state_dict': discriminator.state_dict(),
            }, os.path.join(save_path, f'checkpoint_epoch_{epoch}.pt'))

def main():
    torch.manual_seed(42)
    text_processor = TextProcessor()
    dataset_path = '/content/drive/MyDrive/ColabNotebooks/Signify/preprocessed_signtotextnew_images'
    dataset = SignLanguageDataset(dataset_path, text_processor)
    dataloader = DataLoader(dataset, batch_size=50, shuffle=True, num_workers=2)
    print(f"Dataset size: {len(dataset)} images")
    train_model(dataloader)

if __name__ == "__main__":
    main()

"""# Saving the Model"""

from google.colab import drive
import shutil

# Mount Google Drive
drive.mount('/content/drive')

# Specify the source directory and the destination directory in your Google Drive
source_dir = '/content/model_checkpoints'
destination_dir = '/content/drive/MyDrive/model_checkpoints'

"""## TESTING THE MODEL"""

import torch
import matplotlib.pyplot as plt
from torchvision.utils import make_grid
import torch.nn as nn
from transformers import BertTokenizer, BertModel
import torch.nn.functional as F

class TextProcessor:
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertModel.from_pretrained('bert-base-uncased')
        self.model.eval()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

    def get_embedding(self, text):
        with torch.no_grad():
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(self.device)
            outputs = self.model(**inputs)
            return outputs.last_hidden_state[:, 0, :]

class Generator(nn.Module):
    def __init__(self, bert_embedding_dim=768):
        super(Generator, self).__init__()

        self.embedding_processor = nn.Sequential(
            nn.Linear(bert_embedding_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 512)
        )

        self.main = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, 1, 0, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),

            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(True),

            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(True),

            nn.ConvTranspose2d(16, 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(8),
            nn.ReLU(True),

            nn.ConvTranspose2d(8, 3, 4, 2, 1, bias=False),  # Output should be of size 128x128
            nn.Tanh()
        )

    def forward(self, bert_embedding):
        latent = self.embedding_processor(bert_embedding)
        latent = latent.view(-1, 512, 1, 1)
        return self.main(latent)

def load_trained_model(generator_path, device='cuda'):
    generator = Generator().to(device)
    checkpoint = torch.load(generator_path, map_location=device)
    generator.load_state_dict(checkpoint['generator_state_dict'])
    generator.eval()
    return generator

def generate_image_from_text(text, generator, text_processor, device='cuda'):
    generator.eval()
    with torch.no_grad():
        embedding = text_processor.get_embedding(text).to(device)
        generated_image = generator(embedding)
        return generated_image

def display_image(image_tensor):
    image = (image_tensor * 0.5 + 0.5).clamp(0, 1)  # Denormalize
    plt.figure(figsize=(5, 5))
    plt.imshow(image.cpu().permute(1, 2, 0))
    plt.axis('off')
    plt.show()

def test_model():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    text_processor = TextProcessor()
    generator_path = '/content/drive/MyDrive/model_checkpoints/checkpoint_epoch_99.pt'

    generator = load_trained_model(generator_path, device)

    print("Enter text to generate sign language images, or type 'quit' to exit.")
    while True:
        user_input = input("Text: ")
        if user_input.lower() == 'quit':
            print("Exiting...")
            break

        generated_image = generate_image_from_text(user_input, generator, text_processor, device)
        display_image(generated_image[0])

if __name__ == "__main__":
    test_model()

"""ensuring the data checkpoints are present"""

!ls /content/drive/MyDrive/model_checkpoints

"""nvidia gpu state"""

!nvidia-smi